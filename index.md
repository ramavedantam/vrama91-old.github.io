---
layout: home
title: Home
---

Hi! I am currently a visiting researcher at the NYU Center for Data Science (CDS) working with <a href="https://en.wikipedia.org/wiki/Julia_Kempe">Prof. Julia Kempe</a> and <a href="https://cims.nyu.edu/~andrewgw/">Prof. Andrew Gordon Wilson</a>.
Previously, I was Research Scientist at <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research (FAIR)</a> in New York. I enjoy
working on problems in core machine learning that make AI models more general, robust and human-like. 
In pursuit of these goals, my current focus is on multimodal machine learning, robustness and data efficient learning.

At FAIR I proposed the <a href="https://arxiv.org/abs/2009.12789">Decodable Information Bottleneck for Representation Learning</a> which provides
a theoretical and practical foundation for worst case risk mitigation for deep representation learning.

Previously, I graduated with my PhD in Computer Science from the School of Interactive Computing at Georgia Tech.
My PhD advisor was <a href='https://www.cc.gatech.edu/~parikh/'>Devi Parikh</a>.
My thesis was on <a href="https://smartech.gatech.edu/handle/1853/60799">"Interpretation, Grounding and Imagination for Machine Intelligence"</a>.
Among other things, I proposed the <a href="https://arxiv.org/abs/1411.5726">CIDEr metric</a> commonly used for evaluating image captioning
models (<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=v1CRzeAAAAAJ&citation_for_view=v1CRzeAAAAAJ:9yKSN-GCB0IC">3000+</a> citations) and contributed to the <a href="https://arxiv.org/abs/1610.02391">Grad-CAM</a> method popularly used for interpreting neural
network predictions (<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=v1CRzeAAAAAJ&citation_for_view=v1CRzeAAAAAJ:zYLM7Y9cAGgC">13000+</a> citations) during my Ph.D.

For more details on my research see my <a href='https://scholar.google.com/citations?user=v1CRzeAAAAAJ&hl=en'>Google Scholar</a> page or checkout my <a href="http://vrama91.github.io/publications/">publications</a>. During my PhD, my research was supported by a Google PhD fellowship in Machine Perception, Speech Technology and Computer Vision.

[//]: # "On the vision side, I am interested in problems in vision and language, learning common sense and visual reasoning. On the machine learning side, I am interested in developing tools for effective low-shot learning, generative models, bayesian deep learning and variational inference."

[//]: # "I also care about issues of how we evaluate our models, as we edge towards higher-level AI-complete tasks. In my first project in grad school, I worked on a (now popularly used) evaluation metric for image captioning called CIDEr."

I have been fortunate to work with some great mentors and collaborators during grad school, including <a href="http://larryzitnick.org/">Larry Zitnick</a>,
<a href="http://www.cc.gatech.edu/~dbatra/index.html">Dhruv Batra</a>,
<a href="https://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>,
<a href="http://ai.stanford.edu/~gal/">Gal Chechik</a>, and <a href="http://bengio.abracadoudou.com/">Samy Bengio</a>.

In a previous life, I was an undergrad in ECE at IIIT-Hyderabad where I worked with <a href='http://www.iiit.ac.in/people/faculty/mkrishna'>K. Madhava Krishna</a> in Robotics. <a href='https://sites.google.com/site/ramakrishnavedantam928/'>Here</a> is a link to my old website.

<hr/>

<h3>News</h3>
<ul>
<li> <b>[February, 2023]</b> Paper on Nullspace characterization for Out of Distribution (OOD) generalization accepted at ICLR, 2023!</li>
<li> <b>[February, 2023]</b> Paper on Robustness of VQA models to distribution shifts accepted at CVPR, 2023.</li>
<li> <b>[May, 2022]</b> Paper on Measuring Equivariance of Objectness models accepted at ICML, 2022.</li>
</ul>

<h3>Code</h3>
<ul>
<li> MSCOCO Caption Evaluation <a href="https://github.com/tylin/coco-caption"> code</a></li>
<li> <a href="https://github.com/vrama91/coco-caption">Codes</a> from MSCOCO Caption Evaluation for metrics (BLEU, ROUGE, CIDEr-D and METEOR), independent of the COCO annotations </li>
<li> Code for our CVPR'16 paper on <a href="https://github.com/satwikkottur/VisualWord2Vec">Learning Visually Grounded Word Embeddings</a></li>
<li> Code for our ICLR'18 paper on <a href="https://github.com/google/joint_vae">Generative Models of Visually Grounded Imagination</a></li>
</ul>	
<hr/>
If you like this layout/page, see <a href='demo-post'>this</a> to build your own using github+jekyll 
